{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chaymaebennouri/data-science/blob/main/Problem_set_9_Neural_Networks_I.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQHvkTxnOXN_"
      },
      "source": [
        "**Lecture 9 Neural Networks**\n",
        "\n",
        "Building a simple Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqZ9YcsYOW9q",
        "outputId": "ae221ab9-b05f-4d72-9fa4-c7aecf491b29"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0],\n",
              "       [1],\n",
              "       [1],\n",
              "       [0]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "#input layer (contair the input dataset) :\n",
        "\n",
        "X = np.array([ [0,0,1,0,0],\n",
        "               [1,1,0,1,1],\n",
        "               [1,0,0,1,0],\n",
        "               [0,1,1,1,0]])\n",
        "#Output dataset\n",
        "Y =  np.array ([[0,1,1,0]]).T\n",
        "X\n",
        "Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cX4w3kOOW26"
      },
      "outputs": [],
      "source": [
        "#HIDDEN LAYER ( containing calculations and weights)\n",
        "#Sigmoid function\n",
        "def nonlin(x,deriv=False):\n",
        "  if(deriv == True):\n",
        "    return x*(1-x)\n",
        "  return 1/(1+np.exp(-x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nbCys6LBOWs-"
      },
      "outputs": [],
      "source": [
        "#Seed random numbers to make calculation det erministic (just a good practice)\n",
        "np.random.seed(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bh3y2JCVOWg1",
        "outputId": "b1c38e52-fbfb-4195-d89c-11354d72d52b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.16595599],\n",
              "       [ 0.44064899],\n",
              "       [-0.99977125],\n",
              "       [-0.39533485],\n",
              "       [-0.70648822]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "#Intitialze weights randomly with mean 0\n",
        "weights = 2*np.random.random((5,1)) -1\n",
        "weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_GyxFvo8OV1q"
      },
      "outputs": [],
      "source": [
        "for iter in range(10000):\n",
        "  # Forward propagation\n",
        "  # Input data\n",
        "  l0 = X\n",
        "  # Calculate predicted output\n",
        "  pout=nonlin(np.dot(l0,weights))\n",
        "  # Calculating the cost function:\n",
        "  # This function represent the sum of the error, difference between\n",
        "  # predicted output and the real output value\n",
        "  pout_error=Y-pout\n",
        "  # Multiply prediction  error by the slope of the sigmoid at the values in pout\n",
        "  pout_delta=pout_error*nonlin(pout,True)\n",
        "  # Update weights\n",
        "  weights+=np.dot(l0.T,pout_delta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFYDZlHGTEJ_",
        "outputId": "fcf0cbb0-c67a-42a6-e27e-2182937efc90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input data\n",
            "Input row 0: [0 0 1 0 0]\n",
            "Input row 1: [1 1 0 1 1]\n",
            "Input row 2: [1 0 0 1 0]\n",
            "Input row 3: [0 1 1 1 0]\n",
            "Target data\n",
            "Target neuron 0: [0]\n",
            "Target neuron 1: [1]\n",
            "Target neuron 2: [1]\n",
            "Target neuron 3: [0]\n",
            "Hidden layer weights\n",
            "weight of input-colum 0: [4.26399604]\n",
            "weight of input-colum 1: [-0.79907371]\n",
            "weight of input-colum 2: [-5.43402607]\n",
            "weight of input-colum 3: [1.02167938]\n",
            "weight of input-colum 4: [1.06672687]\n",
            "Predicted Output After Training:\n",
            "Predicted output 0: [0.00434672]\n",
            "Predicted output 1: [0.99614022]\n",
            "Predicted output 2: [0.99496164]\n",
            "Predicted output 3: [0.00542462]\n"
          ]
        }
      ],
      "source": [
        "#PREDICT OUTPUT LAYER\n",
        "print('Input data')\n",
        "print(*('Input row {}: {}'.format(*k) for k in enumerate (l0)), sep=\"\\n\")\n",
        "print(\"Target data\")\n",
        "print(*('Target neuron {}: {}'.format(*k) for k in enumerate (Y)), sep=\"\\n\")\n",
        "print(\"Hidden layer weights\")\n",
        "#Frist layer of weights connecting l0 to put.\n",
        "#when both an input an input and a output are equal,\n",
        "#We increase the weight (strenght of the connection) between them.\n",
        "#when they are different, we decrease the weigth.\n",
        "print(*('weight of input-colum {}: {}'.format(*k) for k in enumerate (weights)), sep=\"\\n\")\n",
        "print(\"Predicted Output After Training:\")\n",
        "print(*('Predicted output {}: {}' .format(*k) for k in enumerate(pout)), sep=\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KM1gnvARU4dc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ws0fNoLeohu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Jt3-39LTDpe"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDZiTQSEbsfB"
      },
      "source": [
        "**Problem set 9**\n",
        "\n",
        "Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2A9WlOg2Ymzy"
      },
      "source": [
        "# Exersise 1:\n",
        "Play with the code you have worked out in the lecture. For example, change the input data and the output data and try to get a good understanding of what is happening and how the neural network adjusts the weights. Then, create a Neural Network with a weight matrix of dimension (6,1).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "#input layer (contair the input dataset) :\n",
        "\n",
        "X = np.array([ [1,0,0,0,1,1],\n",
        "               [0,0,0,1,1,0],\n",
        "               [1,1,0,1,1,1],\n",
        "               [0,1,1,0,0,0],])\n",
        "#Output dataset\n",
        "Y =  np.array ([[0,1,1,0,0,0]]).T\n",
        "X,Y\n",
        "#HIDDEN LAYER ( containing calculations and weights)\n",
        "#Sigmoid function\n",
        "def nonlin(x,deriv=False):\n",
        "  if(deriv == True):\n",
        "    return x*(1-x)\n",
        "  return 1/(1+np.exp(-x))"
      ],
      "metadata": {
        "id": "tQ2RmwyDg2pX"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Seed random numbers to make calculation det erministic (just a good practice)\n",
        "np.random.seed(1)\n",
        "#Intitialze weights randomly with mean 0\n",
        "weights = 2*np.random.random((6,1)) -1\n",
        "weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drgfHsKtg2kM",
        "outputId": "6871ecf1-88ce-4caf-c3d7-54b2b0ca8c6b"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.16595599],\n",
              "       [ 0.44064899],\n",
              "       [-0.99977125],\n",
              "       [-0.39533485],\n",
              "       [-0.70648822],\n",
              "       [-0.81532281]])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "VV3JHLpwYjE4"
      },
      "outputs": [],
      "source": [
        "for iter in range(10000):\n",
        "  # Forward propagation\n",
        "  # Input data\n",
        "  l0 = X\n",
        "  # Calculate predicted output\n",
        "  pout=nonlin(np.dot(l0,weights))\n",
        "  # Calculating the cost function:\n",
        "  # This function represent the sum of the error, difference between\n",
        "  # predicted output and the real output value\n",
        "  #pout_error = Y - pout\n",
        "  # Multiply prediction  error by the slope of the sigmoid at the values in pout\n",
        "  pout_delta = pout_error*nonlin(pout,True)\n",
        "  # Update weights\n",
        "  weights+=np.dot(l0.T,pout_delta)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#PREDICT OUTPUT LAYER\n",
        "print('Input data')\n",
        "print(*('Input row {}: {}'.format(*k) for k in enumerate (l0)), sep=\"\\n\")\n",
        "print(\"Target data\")\n",
        "print(*('Target neuron {}: {}'.format(*k) for k in enumerate (Y)), sep=\"\\n\")\n",
        "print(\"Hidden layer weights\")\n",
        "#Frist layer of weights connecting l0 to put.\n",
        "#when both an input an input and a output are equal,\n",
        "#We increase the weight (strenght of the connection) between them.\n",
        "#when they are different, we decrease the weigth.\n",
        "print(*('weight of input-colum {}: {}'.format(*k) for k in enumerate (weights)), sep=\"\\n\")\n",
        "print(\"Predicted Output After Training:\")\n",
        "print(*('Predicted output {}: {}' .format(*k) for k in enumerate(pout)), sep=\"\\n\")"
      ],
      "metadata": {
        "id": "OoOEtRJthAzo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91e2f8ac-0bdc-407d-c76f-607a3fff0526"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input data\n",
            "Input row 0: [1 0 0 0 1 1]\n",
            "Input row 1: [0 0 0 1 1 0]\n",
            "Input row 2: [1 1 0 1 1 1]\n",
            "Input row 3: [0 1 1 0 0 0]\n",
            "Target data\n",
            "Target neuron 0: [0]\n",
            "Target neuron 1: [1]\n",
            "Target neuron 2: [1]\n",
            "Target neuron 3: [0]\n",
            "Target neuron 4: [0]\n",
            "Target neuron 5: [0]\n",
            "Hidden layer weights\n",
            "weight of input-colum 0: [-1.12803848]\n",
            "weight of input-colum 1: [0.9360102]\n",
            "weight of input-colum 2: [-4.58695836]\n",
            "weight of input-colum 3: [5.33435137]\n",
            "weight of input-colum 4: [-0.02143281]\n",
            "weight of input-colum 5: [-1.7774053]\n",
            "Predicted Output After Training:\n",
            "Predicted output 0: [0.05084626]\n",
            "Predicted output 1: [0.99509586]\n",
            "Predicted output 2: [0.9658874]\n",
            "Predicted output 3: [0.02531181]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-RwhWW3ZdX5"
      },
      "source": [
        "#Exersise 2:\n",
        "What happens if you reduce the number of iterations of your Forward Propagation algorithm?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAf2eXIuZrk9"
      },
      "source": [
        "Reduced the number of iterations in your Forward Propagation algorithm may affect the performance and accuracy of your model.\n",
        "Decreasing the number of iterations will likely speed up the training process. This could be desirable if you are training your model in a real-time setting or have limited computational resources."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ImwfjN3ZsHm"
      },
      "source": [
        "# Exersise 3:\n",
        "Below is a function written in Python. The function takes an input (x) and transforms it into an output (y). Can you explain what kind of transformation it does? Can you think of why it might be useful?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXA5MN5BdES-",
        "outputId": "5a3e5a37-b2e4-444b-f210-a2373dbfe371"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.01521943 0.0413707  0.11245721 0.83095266]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def softmax(x):\n",
        "    \"\"\" applies softmax to an input x\"\"\"\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum()\n",
        "\n",
        "x = np.array([1, 2, 3, 5])\n",
        "y = softmax(x)\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this specific function, we first calculate the exponential of each element in the input x array, and subtract the maximum value of the x array to avoid numerical overflow. Then, we divide each element in the resulting e_x array by the sum of all elements in the e_x array.\n",
        "the softmax function plays a crucial role in the conversion of raw numbers into meaningful probabilities in various machine learning applications."
      ],
      "metadata": {
        "id": "oPw0GXn8_TZd"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}